{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03cd6a6c-052a-4dc1-9ba1-c4133156946c",
   "metadata": {},
   "source": [
    "## 회귀란?\n",
    "### -분류의 논리적 확장으로, 숫자로 표현된 일련의 특징으로부터 실수(연속형 변수) 예측 \n",
    "### -수학적 관점에서 출력값의 수가 무한하기때문에 분류보다 더어려울 수 있음 \n",
    "### -분류문제의 정확도 개념과는 달리 예측값과 실젯값 사이의 오차를 최적화하는 것을 목표로 함 \n",
    "\n",
    "## 활용 사례\n",
    "### -영화 관객수 예측 \n",
    "#### 예고편, 소셜미디어 공유 횟수 기반으로 향후 얼마나 많은 사람이 영화를 볼지 예측\n",
    "### -회사 수익 예측\n",
    "#### 회사의 성장궤도, 시장, 및 계정설을 고려하여 회사의 이익 예측 \n",
    "### -농작물 수확량 예측 \n",
    "#### 특정 지역 정보, 1년 동안의 날씨 정보 등을 기반으로 수확량 예측 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733a5ad6-b7b5-49f5-8bc3-b848683653e6",
   "metadata": {},
   "source": [
    "## MLlib에서 제공하는 회귀모델\n",
    "#### 1. 선형회귀\n",
    "#### 2. 일반화 선형회귀\n",
    "#### 3. 등위회귀\n",
    "#### 4. 의사결정트리\n",
    "#### 5. 랜덤포레스트\n",
    "#### 6. 그래디언트 부스티드 트리\n",
    "#### 7. 생존 회귀\n",
    "\n",
    "## 해당 챕터에서는 아래와 같은 각 모델의 기본사항을 다룰 예정!\n",
    "#### 1. 모델과 알고리즘에 대한 기본 설명\n",
    "#### 2. 모델 설정을 위한 하이퍼파라미터 (모델 초기화 방법)\n",
    "#### 3. 학습 파라미터 (모델 학습에 영향을 주는 파라미터)\n",
    "#### 4. 예측 파라미터 (예측에 영향을 미치는 파라미터)\n",
    "\n",
    "## 모델 확장성 \n",
    "#### MLlib에서 제공하는 회귀모델은 모두 큰 규모의 데이터셋으로 확장 가능\n",
    "#### 모델의 구성, 시스템 규모, 그리고 기타 요소에 다라 달라짐 \n",
    "\n",
    "### *최대 특징 수 | 최대 학습 데이터 수 \n",
    "##### 1. 선형회귀 : 1~1천만개 | 제한 없음\n",
    "##### 2. 일반화 선형 회귀 : 4,096개 | 제한없음\n",
    "##### 3. 등위 회귀 : N/A : 수백만건 \n",
    "##### 4. 의사결정 트리 : 1000개 | 제한 없음 \n",
    "##### 5. 랜덤 포레스트 : 10000개 | 제한없음\n",
    "##### 6. 그래디언트 부스티드 트리 : 1000개 | 제한없음\n",
    "##### 7. 생존 회귀 : 1~천만개 | 제한 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97275eef-39b2-4e64-b45f-0ba0d9c9bbf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T01:41:06.729084Z",
     "iopub.status.busy": "2021-11-22T01:41:06.728381Z",
     "iopub.status.idle": "2021-11-22T01:41:09.692108Z",
     "shell.execute_reply": "2021-11-22T01:41:09.690694Z",
     "shell.execute_reply.started": "2021-11-22T01:41:06.729010Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sample data load \n",
    "df = spark.read.load(\"regression\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e326a76-d06f-4626-8657-40b53de991bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T01:41:43.052629Z",
     "iopub.status.busy": "2021-11-22T01:41:43.051915Z",
     "iopub.status.idle": "2021-11-22T01:41:43.058323Z",
     "shell.execute_reply": "2021-11-22T01:41:43.057063Z",
     "shell.execute_reply.started": "2021-11-22T01:41:43.052480Z"
    }
   },
   "source": [
    "## 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71c43d1-06d4-4cfe-b2cf-bda741e147a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T01:43:01.140452Z",
     "iopub.status.busy": "2021-11-22T01:43:01.139948Z",
     "iopub.status.idle": "2021-11-22T01:43:01.149855Z",
     "shell.execute_reply": "2021-11-22T01:43:01.148271Z",
     "shell.execute_reply.started": "2021-11-22T01:43:01.140389Z"
    }
   },
   "source": [
    "- 입력 특징들의 선형 조합 (가중치가 곱해진 각 특징을 합한 값)이 가우시안 오차와 함께최종 결과로 산출된다고 가정 \n",
    "- 선형 가정 : 단순하게 해석 가능하며, 과적합 되기 힘든 모델로 만들어줌 \n",
    "- 로지스틱 회귀에서처럼  ElasticNet 일반화를 통해 L1 및 L2 일반화 혼합 가능 \n",
    "\n",
    "### 모델 하이퍼파라미터 \n",
    "- 로지스틱 회귀와 동일 (26장 참고)\n",
    "\n",
    "### 학습 하이퍼파라미터\n",
    "- 로지스틱 회귀와 동일 (26장 참고)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0597c9ff-b07f-453c-9595-59cb8b1232c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T01:55:06.566556Z",
     "iopub.status.busy": "2021-11-22T01:55:06.565932Z",
     "iopub.status.idle": "2021-11-22T01:55:07.369155Z",
     "shell.execute_reply": "2021-11-22T01:55:07.367960Z",
     "shell.execute_reply.started": "2021-11-22T01:55:06.566491Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 0.8)\n",
      "epsilon: The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber (default: 1.35)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "loss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 5)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.3)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression().setMaxIter(5).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "print(lr.explainParams())\n",
    "lrModel = lr.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc0e36-a08e-45c0-889e-1b738bdaf9b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T01:50:00.948773Z",
     "iopub.status.busy": "2021-11-22T01:50:00.948096Z",
     "iopub.status.idle": "2021-11-22T01:50:00.957847Z",
     "shell.execute_reply": "2021-11-22T01:50:00.956550Z",
     "shell.execute_reply.started": "2021-11-22T01:50:00.948718Z"
    }
   },
   "source": [
    "- summary : 여러개의 필드로 구성된 요약 객체 반환 \n",
    "    1. 잔차 : 선형 회귀모델로부터 구한 예측값과 실젯값과의 차이 \n",
    "    2. 객체 히스토리 : 반복 학습 과정마다 모델 학습이 어떻게 진행되는지 \n",
    "    3. 평균 제곱근 오차 : 예측값과 실젯값 사이의 거리 계산하여 모델의 성능을 측정하는 척도 \n",
    "    4. R-제곱변수(r2) : 모델에 의해 설명되는 예측 변수의 분산 비율 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e71bc29-60bd-4c9b-9815-6c723f005de1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T01:53:30.583122Z",
     "iopub.status.busy": "2021-11-22T01:53:30.582603Z",
     "iopub.status.idle": "2021-11-22T01:53:30.866573Z",
     "shell.execute_reply": "2021-11-22T01:53:30.865610Z",
     "shell.execute_reply.started": "2021-11-22T01:53:30.583072Z"
    },
    "tags": []
   },
   "source": [
    "df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "60dc4911-8e62-4a0d-9aac-9d673b2d8339",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T02:11:42.249380Z",
     "iopub.status.busy": "2021-11-22T02:11:42.248938Z",
     "iopub.status.idle": "2021-11-22T02:11:42.585115Z",
     "shell.execute_reply": "2021-11-22T02:11:42.584236Z",
     "shell.execute_reply.started": "2021-11-22T02:11:42.249336Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|  0.1280504658561019|\n",
      "|-0.14468269261572098|\n",
      "| -0.4190383262242056|\n",
      "| -0.4190383262242056|\n",
      "|  0.8547088792080308|\n",
      "+--------------------+\n",
      "\n",
      "6\n",
      "[0.5000000000000001, 0.4315295810362787, 0.3132335933881021, 0.312256926665541, 0.3091506081983029, 0.3091505893348027]\n",
      "0.47308424392175985\n",
      "0.720239122691221\n"
     ]
    }
   ],
   "source": [
    "##평가 지표\n",
    "summary = lrModel.summary\n",
    "summary.residuals.show()\n",
    "print (summary.totalIterations)\n",
    "print (summary.objectiveHistory)\n",
    "print (summary.rootMeanSquaredError)\n",
    "print (summary.r2) #1에 가까울수록 회귀 모델이 데이터를ㅇ 더 잘 설명 (결정계수)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a45d7b-ae97-4eb5-86a4-0d317191294a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T01:56:15.456338Z",
     "iopub.status.busy": "2021-11-22T01:56:15.455668Z",
     "iopub.status.idle": "2021-11-22T01:56:15.462266Z",
     "shell.execute_reply": "2021-11-22T01:56:15.460749Z",
     "shell.execute_reply.started": "2021-11-22T01:56:15.456267Z"
    }
   },
   "source": [
    "## 일반화 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f1b958-fac1-428c-a794-81be2d161fdf",
   "metadata": {},
   "source": [
    "- 다른 회귀 모델에 비해 더 세밀하게 제어할 수 있음 \n",
    "- 가우스 (선형 회귀), 이항 (로지스틱 회귀), 푸아송 (푸아송 회귀) 그리고 감마 (감마 회귀) 등 다양한 분포 집합으로부터 예상되는 노이즈 분포를 선택할 수 있음 \n",
    "- 선형 함수와 분포 함수의 평균 사이의 관계를 지정하는 링크 함수 설정을 지원함 \n",
    "- 가우스 : Identity.Log.Inverse\n",
    "- 이항 : Logit.Probit.ClogLog\n",
    "- 푸아송 : Log.Identity.Sqrt\n",
    "- 감마 : Inverse.Identity.Log\n",
    "- 트위디 : 파워 링크 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb84070-2446-456a-b566-ace8628401ed",
   "metadata": {},
   "source": [
    "### 모델 하이퍼 파라미터 \n",
    "1. family : 모델에서 사용할 오차 분포에 대한 설명으로, 가우스, 이항, 푸아송, 감마, 그리고 트위디가 있음\n",
    "2. link : 선형 함수와 분포 함수의 평균 사이의 관계를 지정하는 링크 함수의 이름. 지원되는 옵션 : cloglog, probit, logit, inverse, sqrt, identity, 그리고 log (기본값은 identity) \n",
    "3. solver : 최적화에 사용되는 해찾기 알고리즘. 현재 지원되는 유일한 solver는 irls (반복재가중최소제곱) \n",
    "4. variancePower : 분포의 분산과 평균 사이의 관계를 특징짓는 트위디 분포의 분산 함수에서의 검증력. 트위디에만 적용되며 지원되는 값은 0과 [1,무한). 기본값 =0 \n",
    "5. linkPower : 트위디 family의 파워 링크 함수 색인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab8ff6d-9fe1-446e-9105-eb69f67d7ab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T02:07:59.374748Z",
     "iopub.status.busy": "2021-11-22T02:07:59.374223Z",
     "iopub.status.idle": "2021-11-22T02:07:59.382604Z",
     "shell.execute_reply": "2021-11-22T02:07:59.381389Z",
     "shell.execute_reply.started": "2021-11-22T02:07:59.374686Z"
    },
    "tags": []
   },
   "source": [
    "### 학습 하이퍼파라미터 \n",
    "- 로지스틱 회귀와 동일 (26장 참고)\n",
    "\n",
    "### 예측 파라미터\n",
    "- 하나의 예측 파라미터 제공 \n",
    "- linePredictionCol : 예측에 적용할 각 링크 함수를 정의하는 컬럼명 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e213d6d7-cada-4b3b-8bc6-79ba354226f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T02:09:57.568067Z",
     "iopub.status.busy": "2021-11-22T02:09:57.567315Z",
     "iopub.status.idle": "2021-11-22T02:09:58.184098Z",
     "shell.execute_reply": "2021-11-22T02:09:58.183152Z",
     "shell.execute_reply.started": "2021-11-22T02:09:57.567991Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "family: The name of family which is a description of the error distribution to be used in the model. Supported options: gaussian (default), binomial, poisson, gamma and tweedie. (default: gaussian, current: gaussian)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "link: The name of link function which provides the relationship between the linear predictor and the mean of the distribution function. Supported options: identity, log, inverse, logit, probit, cloglog and sqrt. (current: identity)\n",
      "linkPower: The index in the power link function. Only applicable to the Tweedie family. (undefined)\n",
      "linkPredictionCol: link prediction (linear predictor) column name (current: linkOut)\n",
      "maxIter: max number of iterations (>= 0). (default: 25, current: 10)\n",
      "offsetCol: The offset column name. If this is not set or empty, we treat all instance offsets as 0.0 (undefined)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.3)\n",
      "solver: The solver algorithm for optimization. Supported options: irls. (default: irls)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "variancePower: The power in the variance function of the Tweedie distribution which characterizes the relationship between the variance and mean of the distribution. Only applicable for the Tweedie family. Supported values: 0 and [1, Inf). (default: 0.0)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "glr = GeneralizedLinearRegression()\\\n",
    "  .setFamily(\"gaussian\")\\\n",
    "  .setLink(\"identity\")\\\n",
    "  .setMaxIter(10)\\\n",
    "  .setRegParam(0.3)\\\n",
    "  .setLinkPredictionCol(\"linkOut\")\n",
    "print (glr.explainParams())\n",
    "glrModel = glr.fit(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d3fde0-ab03-45d6-b207-b23ed2b231c7",
   "metadata": {},
   "source": [
    "### 평가지표\n",
    "- R squared : 결정 계수. 적합의 척도\n",
    "- The residuals : 관측 레이블과 예측값의 차이 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a515d214-99fd-4038-9c4c-98896185dd39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T02:17:43.197863Z",
     "iopub.status.busy": "2021-11-22T02:17:43.197159Z",
     "iopub.status.idle": "2021-11-22T02:17:43.211704Z",
     "shell.execute_reply": "2021-11-22T02:17:43.210502Z",
     "shell.execute_reply.started": "2021-11-22T02:17:43.197790Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Coefficients:\n",
       "    Feature Estimate Std Error T Value P Value\n",
       "(Intercept)   0.0867    1.2210  0.0710  0.9549\n",
       " features_0   0.3661    0.7686  0.4764  0.7170\n",
       " features_1   0.0466    0.1380  0.3377  0.7927\n",
       " features_2   0.1831    0.3843  0.4764  0.7170\n",
       "\n",
       "(Dispersion parameter for gaussian family taken to be 0.8466)\n",
       "    Null deviance: 4.0000 on 1 degrees of freedom\n",
       "Residual deviance: 0.8466 on 1 degrees of freedom\n",
       "AIC: 15.3094"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##평가 지표\n",
    "summary = glrModel.summary\n",
    "summary\n",
    "\n",
    "\n",
    "# st_val = sum(map(lambda x: np.power(x,2),y-np.mean(y))) \n",
    "# sse_val = sum(map(lambda x: np.power(x,2),m1.resid_response)) \n",
    "# r2 = 1.0 - sse_val/sst_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121f8708-c1c7-4170-a0fb-39d30c6170c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T02:20:11.568955Z",
     "iopub.status.busy": "2021-11-22T02:20:11.567973Z",
     "iopub.status.idle": "2021-11-22T02:20:11.576309Z",
     "shell.execute_reply": "2021-11-22T02:20:11.575065Z",
     "shell.execute_reply.started": "2021-11-22T02:20:11.568838Z"
    }
   },
   "source": [
    "## 의사결정트리 \n",
    "- 분류에 적용된 의사결정 트리와 상당히 유사하게 작동\n",
    "- 차이점은 회귀의 의사결정트리는 분류의 레이블 대신 말단 노드마다 하나의 숫자를 출력 \n",
    "- 함수를 모델링하기 위해 계수를 학습하는 대신 단순히 수치를 예측하는 트리를 만듦\n",
    "- 선형 회구와 달리 입력 데이터의 비선형 패턴을 예측할 수 있기 때문에 유의미하나, 과적합될 수 있는 위험이 높음 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6079fe40-c2a7-45e9-9b7b-adb373a0be2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T02:21:25.569260Z",
     "iopub.status.busy": "2021-11-22T02:21:25.568601Z",
     "iopub.status.idle": "2021-11-22T02:21:25.577965Z",
     "shell.execute_reply": "2021-11-22T02:21:25.576539Z",
     "shell.execute_reply.started": "2021-11-22T02:21:25.569200Z"
    }
   },
   "source": [
    "### 모델 하이퍼 파라미터 \n",
    "- impurity 파라미터의 경미한 변경을 제외하고는 분류와 동일 \n",
    "- impurity: 모델이 특정 값을 갖는 특정 말단 노드에서 분할되어야 하는지 아니면 그대로 유지되어야 하는지 여부에 대한 평가지표를 의미. 회귀트리에 지원되는 유일한 평가지표는 '분산'\n",
    "\n",
    "### 학습 하이퍼 파라미터 \n",
    "- 26.4.2절 '학습 파라미터' 참조 -> 분류와 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bbab5753-0ee5-484b-974f-00c7a70db079",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T02:22:33.857505Z",
     "iopub.status.busy": "2021-11-22T02:22:33.856793Z",
     "iopub.status.idle": "2021-11-22T02:22:35.071878Z",
     "shell.execute_reply": "2021-11-22T02:22:35.070642Z",
     "shell.execute_reply.started": "2021-11-22T02:22:33.857430Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "featuresCol: features column name. (default: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)\n",
      "labelCol: label column name. (default: label)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: -478041667385973526)\n",
      "varianceCol: column name for the biased sample variance of prediction. (undefined)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "dtr = DecisionTreeRegressor()\n",
    "print (dtr.explainParams())\n",
    "dtrModel = dtr.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b318f3-b41e-4815-97df-acfb3178869a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T02:24:21.304253Z",
     "iopub.status.busy": "2021-11-22T02:24:21.303611Z",
     "iopub.status.idle": "2021-11-22T02:24:21.314755Z",
     "shell.execute_reply": "2021-11-22T02:24:21.313383Z",
     "shell.execute_reply.started": "2021-11-22T02:24:21.304186Z"
    },
    "tags": []
   },
   "source": [
    "## 랜덤포레스트와 그래디언트 부스티드 트리 \n",
    "- 랜덤 포레스트와 그래디언트 부스티드 트리 모델은 분류 및 회귀 모델에 모두 적용 가능 \n",
    "- 의사결정트리와 동일한 개념이지만 회귀를 수행하는 여러개의 트리 학습\n",
    "- 랜덤 포레스트는 상관관계가 없는 다양한 의사결정트리가 학습되고 평균화되며, 그래디언트에서는 개별 트리가 학습될 때 별도의 가주잋 부여 \n",
    "- 순도(purity) 척도를 제외하면 일반 분류모델과 동일한 모델/학습 파라미터 가짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c5ba0c43-ba52-4396-9b45-f9793e2281a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T02:25:21.566288Z",
     "iopub.status.busy": "2021-11-22T02:25:21.565764Z",
     "iopub.status.idle": "2021-11-22T02:25:29.945804Z",
     "shell.execute_reply": "2021-11-22T02:25:29.944931Z",
     "shell.execute_reply.started": "2021-11-22T02:25:21.566240Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: auto)\n",
      "featuresCol: features column name. (default: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)\n",
      "labelCol: label column name. (default: label)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "numTrees: Number of trees to train (>= 1). (default: 20)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: -2174447743797072350)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n",
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: all)\n",
      "featuresCol: features column name. (default: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)\n",
      "labelCol: label column name. (default: label)\n",
      "lossType: Loss function which GBT tries to minimize (case-insensitive). Supported options: squared, absolute (default: squared)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxIter: max number of iterations (>= 0). (default: 20)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: 7404623767608519207)\n",
      "stepSize: Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator. (default: 0.1)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "rf =  RandomForestRegressor()\n",
    "print (rf.explainParams())\n",
    "rfModel = rf.fit(df)\n",
    "gbt = GBTRegressor()\n",
    "print (gbt.explainParams())\n",
    "gbtModel = gbt.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75403d57-f59d-45b4-8348-d41365ed4d35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T02:30:24.565729Z",
     "iopub.status.busy": "2021-11-22T02:30:24.565141Z",
     "iopub.status.idle": "2021-11-22T02:30:24.573774Z",
     "shell.execute_reply": "2021-11-22T02:30:24.572188Z",
     "shell.execute_reply.started": "2021-11-22T02:30:24.565669Z"
    },
    "tags": []
   },
   "source": [
    "## 고급 방법론\n",
    "1. 생존 회귀 (가속 수명 시간 모델)\n",
    "- 통제된 실험 환경에서 생존 분석으로 개인의 생존율 이해 \n",
    "- 생존 시간의 로그를 모델화 하는 가속 수명 시간 모델을 구현 \n",
    "- 스파크에서 잘 알려진 콕스 비례 위험 모델 대신 생존 회귀 모델을 사용함 (콕스 비례 위험 모델은 준모수적이고, 대용량 데이터셋 확장이 어려움)\n",
    "- 중도전달 변수를 도입해야 함 (피험자가 실험을 그만두는 경우, 연구자에게 상태가 전달되지 않은 피험자의 최종 상태를 추정할 수 없기 때문에 사용)\n",
    "    - 생존 분석에서 손실된 데이터를 처리하는 방법 \n",
    "\n",
    "2. 등위 회귀 \n",
    "- 특별한 요구사항이 존재하는 또 다른 특수한 회귀 모델 \n",
    "- 항상 단조롭게 증가하는 구간적 선형 함수를 지정한다. (즉 감소 불가능)\n",
    "- 데이터가 우상향하는 추이를 보이면 모델이 적절하다는 의미를 가짐. 입력값이 변화함에 따라 우상향 추이를 보이지 않는다면 올바른 모델이라 할 수 없음 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c817fe66-ba73-4f5e-baf7-969b46a98953",
   "metadata": {},
   "source": [
    "## 평가기와 모델 튜닝 자동화 \n",
    "- 분류 문제에서 살펴본 코어 모델 튜닝 기능을 동일하게 제공 \n",
    "- 평가기를 지정하고 최적화할 수 있는 지표 선택 후, 파이프라인을 학습시켜 파라미터를 튜닝함 \n",
    "- 회귀 분석을 위한 평가기는 RegressionEvaluator라고 부르며, 일반적인 회귀성과측정 평가지표를 최적화시킴\n",
    "- 예측값과 실젯값을 나타내는 두개의 컬럼 제공 \n",
    "- 최적화를 위해 지원되는 평가지표 : 평균제곱근오차(rsme), 평균제곱오차(mse), 결정계수(r2), 평균절대오차(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f542bb62-f67f-42f6-bc7d-edcded8167d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T02:39:01.184981Z",
     "iopub.status.busy": "2021-11-22T02:39:01.184362Z",
     "iopub.status.idle": "2021-11-22T02:39:04.171315Z",
     "shell.execute_reply": "2021-11-22T02:39:04.170031Z",
     "shell.execute_reply.started": "2021-11-22T02:39:01.184917Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "glr = GeneralizedLinearRegression().setFamily(\"gaussian\").setLink(\"identity\")\n",
    "pipeline = Pipeline().setStages([glr])\n",
    "params = ParamGridBuilder().addGrid(glr.regParam, [0.0, 0.5, 1.0]).build()\n",
    "evaluator = RegressionEvaluator()\\\n",
    "  .setMetricName(\"rmse\")\\\n",
    "  .setPredictionCol(\"prediction\")\\\n",
    "  .setLabelCol(\"label\")\n",
    "cv = CrossValidator()\\\n",
    "  .setEstimator(pipeline)\\\n",
    "  .setEvaluator(evaluator)\\\n",
    "  .setEstimatorParamMaps(params)\\\n",
    "  .setNumFolds(2) # should always be 3 or more but this dataset is small\n",
    "model = cv.fit(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af23e091-94f3-49a6-8567-cbac75973337",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T02:48:37.049082Z",
     "iopub.status.busy": "2021-11-22T02:48:37.048606Z",
     "iopub.status.idle": "2021-11-22T02:48:37.059576Z",
     "shell.execute_reply": "2021-11-22T02:48:37.057769Z",
     "shell.execute_reply.started": "2021-11-22T02:48:37.049035Z"
    }
   },
   "source": [
    "## 평가지표 \n",
    "- 평가기를 사용하면 하나의 특정한 평가지표에 따라 모델을 평가하고 적합시킬 수 있음 \n",
    "- RegressionMetrics 객체를 통해 추가로 다양한 회귀 평가지표를 검토할 수 있음\n",
    "    - (예측, 레이블 ) 쌍의 RDD를 기반으로 동작 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d03443cd-a63d-4828-a3ee-3c5e439005be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T02:49:37.950486Z",
     "iopub.status.busy": "2021-11-22T02:49:37.950070Z",
     "iopub.status.idle": "2021-11-22T02:49:43.896754Z",
     "shell.execute_reply": "2021-11-22T02:49:43.895369Z",
     "shell.execute_reply.started": "2021-11-22T02:49:37.950444Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.18038162667943697\n",
      "RMSE: 0.42471358193426895\n",
      "R-squared: 0.7745229666507039\n",
      "MAE: 0.3277600580484718\n",
      "Explained variance: 0.4448396208322792\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "out = model.transform(df)\\\n",
    "  .select(\"prediction\", \"label\").rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
    "metrics = RegressionMetrics(out)\n",
    "print (\"MSE: \" + str(metrics.meanSquaredError))\n",
    "print (\"RMSE: \" + str(metrics.rootMeanSquaredError))\n",
    "print (\"R-squared: \" + str(metrics.r2))\n",
    "print (\"MAE: \" + str(metrics.meanAbsoluteError))\n",
    "print (\"Explained variance: \" + str(metrics.explainedVariance))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark-driver-mem-16g",
   "language": "python",
   "name": "pyspark-driver-mem-16g"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
