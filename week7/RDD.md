# 12장. RDD

Spark의 저수준 API는 RDD, SparkContext, Accumulator, 브로드캐스트 변수와 같은 분산형 공유 변수
RDD 분산 데이터 처리를 위한 RDD / Accumulator, 브로드캐스트 분산형 공유 변수를 배포하고 다루기 위한 API 2개의 저수준 API

12.1.1 저수준 API가 필요한 경우
- 고수준API에서 제공하지 않는기능이필요한경우, 예를들어 클러스터의 물리적데이터의 배치를 아주 세밀하게 제어 해야하는 싱황에서는 저수준API가 필요.
- RDD를 사용해 개발된 기존 코드를 유지해야 하는 경우
- 사용자가 정의한 공유변(14장)를 다뤄야 하는경우
- 이전 버전의 스파크에서 자체 구현한 파티셔너를 시용하거나 데이터 파이프라인이 실행되는 동안 변수값을 갱신하고 추적하는데 저수준 API가 필요한 경우도 有 
- 저수준 API는 세밀한 제어 방법을 제공하여 개빌자가 치명적인 실수를 하지 않도록 도외주기도 하므로 사용함

12.2.2 저수준 API는 어떻게 사용할까 ? 
- SparkContext(15장)는 저수준 API 기능을 사용하기 위한 진입 지점입니다. SparkContext에 접근하는 방법은 spark.sparkContext 코드 사용

### 12.2 RDD 개요

- 모든 Dataframe, Dataset 코드는 RDD로 컴파일됨. 스파크 UI에서 RDD 단위로 잡이 수행됨 -> RDD 이해 필요
- RDD는 불변성을 가지며 병렬로 처리할 수 있는 파티셔닝된 레코드의 모음
- DataFrame의 각 레코드는 스키마를 일고 있는 필드로 구성된 구조화된 로우인 반면, RDD의 레코드는 그저 프로그래머가 선택히는 자바, 스칼라, 파이썬의 객체일 뿐입니다.
- 이 객체에는 사용자가 원하는 포맷을 사용해 원하는 모든 데이터를 저장 할 수 있음
- 구조적 API와는 다르게 레코드의 내부 구조를 스파크에서 피악할 수 없으므로 최적화를 하려면 훨씬 많은 수작업이 필요
- RDD API는 11장에서 알아본 Dataset과 유사하지만 RDD는 구조화된 데이터 엔진을 시용해 데이터를 저장하거나 다루지 않음. 하지만 RDD와 Dataset 사이의 전환은 매우 쉬우므로
두 API를 모두 사용해 각 API의 장점을 동시에 활용할 수 있습니다.

- 주요 속성
  - 파티션의 목록
  - 각 조각을 연산하는 힘수
  - 다른 RDD와의 의존성 목록
  - 부가적으로 키 값 RDD를 위한 Partitioner (예 : RDD는 해쉬 파티셔닝되어 있다고 말할 수 있습니다)
  - 부가적으로 각 조각을 연산하기 위한 기본 위치 목록(예 : 하둡 분산 파일 시스템 파일의 블록 위치)

12.2.1 유형
- RDD는 DataFrame API에서 최적화된 물리적 실행 계획을 만드는 데 대부분 사용
- (1) 제네릭 RDD type
- (2) 키 기반의 집계가 가능한 키-값 RDD : 특수 연산 뿐만 아니라 키를 이용한 사용자 지정 파티셔닝 개념 보유
