
# 16장. 스파크 애플리케이션 개발하기  

-이전 장! 스파크가 클러스터 환경에서 어떻게 스파크 코드를 실행하는지
-이번 장! 표준 스파크 애플리케이션을 개발하고 클러스터에 배포하는 과정 

## 16.1 스파크 애플리케이션 작성하기 
-스파크 애플리케이션은 스파크 클러스터와 사용자 코드 두가지 조합으로 구성 

+ 16.1.1간단한 스칼라 기반 앱
  + 스파크 애플리케이션은 자바가상머신 기반의 빌드 도구인 sbt나 아파치 메이븐을 이용해 빌드 
  + sbt를 사용하는 것이 더 쉬움 
  + sbt 빌드 환경 구성을 위해 build.sbt파일 정의 필요 
    + 프로젝트 메타데이터(패키지명, 패키지 버전 정보 등)
    + 라이브러리 의존성을 관리하는 장소 
    + 라이브러리에 포함된 의존성 정보 
  + sbt 홈페이지에서 관련 정보 찾기 가능 : www.scala-sbt.org/1.x/docs/Basic-Def.html

``` 
name := "example" // change to project name
organization := "com.databricks" // change to your org
version := "0.1-SNAPSHOT"
scalaVersion := "2.11.8"

// Spark Information
val sparkVersion = "2.1.0"

// allows us to include spark packages
resolvers += "bintray-spark-packages" at
  "https://dl.bintray.com/spark-packages/maven/"

resolvers += "Typesafe Simple Repository" at
  "http://repo.typesafe.com/typesafe/simple/maven-releases/"

resolvers += "MavenRepository" at
  "https://mvnrepository.com/"

libraryDependencies ++= Seq(
  // spark core
  "org.apache.spark" %% "spark-core" % sparkVersion,
  "org.apache.spark" %% "spark-sql" % sparkVersion,

  // spark-modules
  "org.apache.spark" %% "spark-graphx" % sparkVersion,
  // "org.apache.spark" %% "spark-mllib" % sparkVersion,

  // spark packages
  "graphframes" % "graphframes" % "0.4.0-spark2.1-s_2.11",

  // testing
  "org.scalatest" %% "scalatest" % "2.2.4" % "test",
  "org.scalacheck" %% "scalacheck" % "1.12.2" % "test",

  // logging
  "org.apache.logging.log4j" % "log4j-api" % "2.4.1",
  "org.apache.logging.log4j" % "log4j-core" % "2.4.1"
)

  ```
  + build.sbt파일 정의 후 sbt메뉴얼의 포준 스칼라 프로젝트 구조 작성 ( 메이븐 프로젝트도 동일 구조)
  
  ``` 
  src/
  main/
    resources/
       <JAR 파일에 포함할 파일들>
    scala/
       <메인 스칼라 소스 파일>
    java/
       <메인 자바 소스 파일>
  test/
    resources
       <테스트 JAR에 포함할 파일들>
    scala/
       <테스트용 스칼라 소스 파일>
    java/
       <테스트용 자바 소스 파일>
  ```
  + 스칼라와 자바 디렉터리에 아래 소스코드 작성 
    + SparkSession을 초기화 -> 애플리케이션 실행 -> 종료 
    
  ```

    object DataFrameExample extends Serializable {
  def main(args: Array[String]) = {

    val pathToDataFolder = args(0)

    // 설정값 지정 -> SparkSession 시작
    val spark = SparkSession.builder().appName("Spark Example")
      .config("spark.sql.warehouse.dir", "/user/hive/warehouse")
      .getOrCreate()

    // udf 등록
    spark.udf.register("myUDF", someUDF(_:String):String)
    
    val df = spark.read.json(pathToDataFolder + "data.json")
    val manipulated = df.groupBy(expr("myUDF(group)")).sum().collect()
     .foreach(x => println(x))
  }
}

  ```
