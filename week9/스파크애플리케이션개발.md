
# 16장. 스파크 애플리케이션 개발하기  

-이전 장! 스파크가 클러스터 환경에서 어떻게 스파크 코드를 실행하는지
-이번 장! 표준 스파크 애플리케이션을 개발하고 클러스터에 배포하는 과정 

## 16.1 스파크 애플리케이션 작성하기 
-스파크 애플리케이션은 스파크 클러스터와 사용자 코드 두가지 조합으로 구성 

### 간단한 스칼라 기반 앱 
+ 스파크 애플리케이션은 자바가상머신 기반의 빌드 도구인 sbt나 아파치 메이븐을 이용해 빌드 
  + sbt를 사용하는 것이 더 쉬움 
  + sbt 빌드 환경 구성을 위해 build.sbt파일 정의 필요 
    + 프로젝트 메타데이터(패키지명, 패키지 버전 정보 등)
    + 라이브러리 의존성을 관리하는 장소 
    + 라이브러리에 포함된 의존성 정보 
  + sbt 홈페이지에서 관련 정보 찾기 가능 : www.scala-sbt.org/1.x/docs/Basic-Def.html

``` C
name := "example" // change to project name
organization := "com.databricks" // change to your org
version := "0.1-SNAPSHOT"
scalaVersion := "2.11.8"

// Spark Information
val sparkVersion = "2.1.0"

// allows us to include spark packages
resolvers += "bintray-spark-packages" at
  "https://dl.bintray.com/spark-packages/maven/"

resolvers += "Typesafe Simple Repository" at
  "http://repo.typesafe.com/typesafe/simple/maven-releases/"

resolvers += "MavenRepository" at
  "https://mvnrepository.com/"

libraryDependencies ++= Seq(
  // spark core
  "org.apache.spark" %% "spark-core" % sparkVersion,
  "org.apache.spark" %% "spark-sql" % sparkVersion,

  // spark-modules
  "org.apache.spark" %% "spark-graphx" % sparkVersion,
  // "org.apache.spark" %% "spark-mllib" % sparkVersion,

  // spark packages
  "graphframes" % "graphframes" % "0.4.0-spark2.1-s_2.11",

  // testing
  "org.scalatest" %% "scalatest" % "2.2.4" % "test",
  "org.scalacheck" %% "scalacheck" % "1.12.2" % "test",

  // logging

"org.apache.logging.log4j" % "log4j-api" % "2.4.1",
  "org.apache.logging.log4j" % "log4j-core" % "2.4.1"
) 
```

+ build.sbt파일 정의 후 sbt메뉴얼의 포준 스칼라 프로젝트 구조 작성 ( 메이븐 프로젝트도 동일 구조)
    
    
  ``` C
  src/
  main/
    resources/
       <JAR 파일에 포함할 파일들>
    scala/
       <메인 스칼라 소스 파일>
    java/
       <메인 자바 소스 파일>
  test/
    resources
       <테스트 JAR에 포함할 파일들>
    scala/
       <테스트용 스칼라 소스 파일>
    java/
       <테스트용 자바 소스 파일> ```  
       
+ 스칼라와 자바 디렉터리에 아래 소스코드 작성 
+ SparkSession을 초기화 -> 애플리케이션 실행 -> 종료 
    
 ``` C
    object DataFrameExample extends Serializable {
  def main(args: Array[String]) = {

    val pathToDataFolder = args(0)

    // 설정값 지정 -> SparkSession 시작
    val spark = SparkSession.builder().appName("Spark Example")
      .config("spark.sql.warehouse.dir", "/user/hive/warehouse")
      .getOrCreate()

    // udf 등록
    spark.udf.register("myUDF", someUDF(_:String):String)
    
    val df = spark.read.json(pathToDataFolder + "data.json")
    val manipulated = df.groupBy(expr("myUDF(group)")).sum().collect()
     .foreach(x => println(x))
  }
}  

```

+ spark-submit 명령을 사용해 예제 코드를 클러스터에 제출 
+ 실행 명령 중 main 클래스 지정한 부분 중요! 
+ 하나의 jAR 파일 안에 관련 라이브러리 모두를 포함하는 uber-jar 또는 fat-jar로 빌드하기 위해 sbt assemble 명령 사용 
+ 라이브러리 의존성 충돌하는 경우가 생길 수 있음 
+ 가장 쉬운 빌드 방법은 sbt package 명령을 실행하는 것 -> 관련 라이브러리 모두를 target 폴더에 모아줌 
  + 간단한 예제sms 필요한 library가 하나 뿐이지만, 보통의 경우 하나의 Application을 개발하기 위해서는 상당히 많은 library를 사용하게 됨 
  + 이런 Application을 배포하기 위해서는 위에서처럼, 사용된 library의 jar 파일과 classpath를 함께 제공해 주어야 하는데, 몇 십개씩되는 library를 일일이 관리하는 것은 어려움 
  + 따라서 .sbt파일에 addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.4") 플러그인 이름과 버전 추가 
  + shell에서 assembly라는 명령어 사용 
  + jar tf test-assembly-1.0.jar내용 확인 -> fat jar로 불림 
    + Application을 구동하는데 필요한 Class 파일들을 하나의 jar 내부에 모두 가지고 있음 

``` C

jae/
jae/lee/
jae/lee/example/
scala/
scala/annotation/
scala/annotation/meta/
scala/annotation/unchecked/
scala/beans/
scala/collection/
scala/collection/concurrent/
scala/collection/convert/
scala/collection/generic/
<중략>

``` 

+ target 폴더에 saprk-submit이라는 인수로 사용할 jar 파일이 들어 있음 
+ 스칼라 패키지 빌드 후, saprk-submit 명령을 사용해 로컬머신에서 실행 가능 

``` C
$SPARK_HOME/bin/spark-submit \
  --class com.databricks.example.DataFrameExample \
  --master local \
  target/scala-2.11/example_2.11-0.1-SNAPSHOT.jar "hello"
``` 

### 파이썬 애플리케이션 작성 
+  일반 파이썬 애플리케이션이나 패키지를 작성하는 방법과 유사 
+  스파크에는 빌드 개념이 없으며, Pyspark 애플리케이션은 파이썬 스크립트와 동일 
  +  애플리케이션 실행 = 스크립트 실행
+ 코드 재사용을 위해 여러 파이썬 파일을 하나의 egg나 ZIP 파일 형태로 압축 
+ spark-submit의 --py-files 인수로 .py, .zip, .egg파일을 지정하면 배포 가능 
1. '스칼라나 자바의 메인 클래스' 역할을 하는 파이썬 파일 작성 (sparkSession을 생성하는 실행 가능한 스크립트 파일) 
``` C
# 파이썬 코드
from __future__ import print_function

if __name__ == '__main__':
  from pyspark.sql import SparkSession
  spark = SparkSession.builder \
    .master("local") \
    .appName("Word Count") \
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()
  
  print(spark.range(5000).where("id>500").selectExpr("sum(id)").collect())
```
2. 라이브러리 의존성을 정의하기 위해 pip 사용 가능 (pip install pyspark 명령으로 설치 -> 다른 패키지와 같은 방식으로 의존성 정의)
3. 애플리케이션 실행 
``` C
$SPARK_HOME/bin/spark-submit --master local pyspark_template/main.py
``` 
### 자바 애플리케이션 작성 
+ 스칼라의 작성 방법과 매우 유사 
+ 차이점은 사용자가 메이븐을 사용해 라이브러리 의존성을 지정
  + 메이븐 형식 
``` C
<dependencies>
  <dependency>
    <groupId>junit</groupId>
    <artifactId>junit</artifactId>
    <version>3.8.1</version>
    <scope>test</scope>
  </dependency>
  <dependency> <!-- Spark dependency -->
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-sql_2.11</artifactId>
    <version>2.4.0</version>
  </dependency>
  <dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming_2.11</artifactId>
    <version>2.4.0</version>
  </dependency>
  <dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming-kafka-0-10_2.11</artifactId>
    <version>2.4.0</version>
  </dependency>
  <dependency>
    <groupId>org.json</groupId>
    <artifactId>json</artifactId>
    <version>20180813</version>
  </dependency>
</dependencies>
<repositories>
  <id>SparkPackagesRepo</id>
  <url>http://dl.bintray.com/spark-packages/maven</url>
</repositories>
``` 

+ java main 클래스 코드 예제 

``` C
import org.apache.spark.sql.SparkSession;

public class SimpleExample{
  public static void main(String[] args){
    SparkSession spark = SparkSession.builder().getOrCreate();
    spark.range(1,2000).count();
  }
}
``` 
+애플리케이션 실행 
``` C
$SPARK_HOME/bin/spark-submit \
  --class com.databricks.example.SimpleExample \
  --master local \
  target/spark-example-0.1-SNAPSHOT.jar "hello"
``` 
 
