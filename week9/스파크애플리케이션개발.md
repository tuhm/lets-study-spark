
# 16장. 스파크 애플리케이션 개발하기  

-이전 장! 스파크가 클러스터 환경에서 어떻게 스파크 코드를 실행하는지
-이번 장! 표준 스파크 애플리케이션을 개발하고 클러스터에 배포하는 과정 

## 16.1 스파크 애플리케이션 작성하기 
-스파크 애플리케이션은 스파크 클러스터와 사용자 코드 두가지 조합으로 구성 

### 간단한 스칼라 기반 앱 
+ 스파크 애플리케이션은 자바가상머신 기반의 빌드 도구인 sbt나 아파치 메이븐을 이용해 빌드 
  + sbt를 사용하는 것이 더 쉬움 
  + sbt 빌드 환경 구성을 위해 build.sbt파일 정의 필요 
    + 프로젝트 메타데이터(패키지명, 패키지 버전 정보 등)
    + 라이브러리 의존성을 관리하는 장소 
    + 라이브러리에 포함된 의존성 정보 
  + sbt 홈페이지에서 관련 정보 찾기 가능 : www.scala-sbt.org/1.x/docs/Basic-Def.html

``` C
name := "example" // change to project name
organization := "com.databricks" // change to your org
version := "0.1-SNAPSHOT"
scalaVersion := "2.11.8"

// Spark Information
val sparkVersion = "2.1.0"

// allows us to include spark packages
resolvers += "bintray-spark-packages" at
  "https://dl.bintray.com/spark-packages/maven/"

resolvers += "Typesafe Simple Repository" at
  "http://repo.typesafe.com/typesafe/simple/maven-releases/"

resolvers += "MavenRepository" at
  "https://mvnrepository.com/"

libraryDependencies ++= Seq(
  // spark core
  "org.apache.spark" %% "spark-core" % sparkVersion,
  "org.apache.spark" %% "spark-sql" % sparkVersion,

  // spark-modules
  "org.apache.spark" %% "spark-graphx" % sparkVersion,
  // "org.apache.spark" %% "spark-mllib" % sparkVersion,

  // spark packages
  "graphframes" % "graphframes" % "0.4.0-spark2.1-s_2.11",

  // testing
  "org.scalatest" %% "scalatest" % "2.2.4" % "test",
  "org.scalacheck" %% "scalacheck" % "1.12.2" % "test",

  // logging

"org.apache.logging.log4j" % "log4j-api" % "2.4.1",
  "org.apache.logging.log4j" % "log4j-core" % "2.4.1"
) 
```

+ build.sbt파일 정의 후 sbt메뉴얼의 포준 스칼라 프로젝트 구조 작성 ( 메이븐 프로젝트도 동일 구조)
    
    
  ``` C
  src/
  main/
    resources/
       <JAR 파일에 포함할 파일들>
    scala/
       <메인 스칼라 소스 파일>
    java/
       <메인 자바 소스 파일>
  test/
    resources
       <테스트 JAR에 포함할 파일들>
    scala/
       <테스트용 스칼라 소스 파일>
    java/
       <테스트용 자바 소스 파일> ```  
       
+ 스칼라와 자바 디렉터리에 아래 소스코드 작성 
  + SparkSession을 초기화 -> 애플리케이션 실행 -> 종료 
    
 ``` C
    object DataFrameExample extends Serializable {
  def main(args: Array[String]) = {

    val pathToDataFolder = args(0)

    // 설정값 지정 -> SparkSession 시작
    val spark = SparkSession.builder().appName("Spark Example")
      .config("spark.sql.warehouse.dir", "/user/hive/warehouse")
      .getOrCreate()

    // udf 등록
    spark.udf.register("myUDF", someUDF(_:String):String)
    
    val df = spark.read.json(pathToDataFolder + "data.json")
    val manipulated = df.groupBy(expr("myUDF(group)")).sum().collect()
     .foreach(x => println(x))
  }
}  

```

+ spark-submit 명령을 사용해 예제 코드를 클러스터에 제출 
+ 실행 명령 중 main 클래스 지정한 부분 중요! 
+ 하나의 jAR 파일 안에 관련 라이브러리 모두를 포함하는 uber-jar 또는 fat-jar로 빌드하기 위해 sbt assemble 명령 사용 
+ 라이브러리 의존성 충돌하는 경우가 생길 수 있음 
+ 가장 쉬운 빌드 방법은 sbt package 명령을 실행하는 것 -> 관련 라이브러리 모두를 target 폴더에 모아줌 
  + 간단한 예제sms 필요한 library가 하나 뿐이지만, 보통의 경우 하나의 Application을 개발하기 위해서는 상당히 많은 library를 사용하게 됨 
  + 이런 Application을 배포하기 위해서는 위에서처럼, 사용된 library의 jar 파일과 classpath를 함께 제공해 주어야 하는데, 몇 십개씩되는 library를 일일이 관리하는 것은 어려움 
  + 따라서 .sbt파일에 addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.4") 플러그인 이름과 버전 추가 
  + shell에서 assembly라는 명령어 사용 
  + jar tf test-assembly-1.0.jar내용 확인 -> fat jar로 불림 
    + Application을 구동하는데 필요한 Class 파일들을 하나의 jar 내부에 모두 가지고 있음 

``` C

jae/
jae/lee/
jae/lee/example/
scala/
scala/annotation/
scala/annotation/meta/
scala/annotation/unchecked/
scala/beans/
scala/collection/
scala/collection/concurrent/
scala/collection/convert/
scala/collection/generic/
<중략>

``` 

+ target 폴더에 saprk-submit이라는 인수로 사용할 jar 파일이 들어 있음 
+ 스칼라 패키지 빌드 후, saprk-submit 명령을 사용해 로컬머신에서 실행 가능 

``` C
$SPARK_HOME/bin/spark-submit \
  --class com.databricks.example.DataFrameExample \
  --master local \
  target/scala-2.11/example_2.11-0.1-SNAPSHOT.jar "hello"
``` 

### 파이썬 애플리케이션 작성 
+  일반 파이썬 애플리케이션이나 패키지를 작성하는 방법과 유사 
+  스파크에는 빌드 개념이 없으며, Pyspark 애플리케이션은 파이썬 스크립트와 동일 
  +  애플리케이션 실행 = 스크립트 실행
+ 코드 재사용을 위해 여러 파이썬 파일을 하나의 egg나 ZIP 파일 형태로 압축 
+ spark-submit의 --py-files 인수로 .py, .zip, .egg파일을 지정하면 배포 가능 
1. '스칼라나 자바의 메인 클래스' 역할을 하는 파이썬 파일 작성 (sparkSession을 생성하는 실행 가능한 스크립트 파일) 
``` C
# 파이썬 코드
from __future__ import print_function

if __name__ == '__main__':
  from pyspark.sql import SparkSession
  spark = SparkSession.builder \
    .master("local") \
    .appName("Word Count") \
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()
  
  print(spark.range(5000).where("id>500").selectExpr("sum(id)").collect())
```
2. 라이브러리 의존성을 정의하기 위해 pip 사용 가능 (pip install pyspark 명령으로 설치 -> 다른 패키지와 같은 방식으로 의존성 정의)
3. 애플리케이션 실행 
``` C
$SPARK_HOME/bin/spark-submit --master local pyspark_template/main.py
``` 
### 자바 애플리케이션 작성 
+ 스칼라의 작성 방법과 매우 유사 
+ 차이점은 사용자가 메이븐을 사용해 라이브러리 의존성을 지정
  + 메이븐 형식 
``` C
<dependencies>
  <dependency>
    <groupId>junit</groupId>
    <artifactId>junit</artifactId>
    <version>3.8.1</version>
    <scope>test</scope>
  </dependency>
  <dependency> <!-- Spark dependency -->
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-sql_2.11</artifactId>
    <version>2.4.0</version>
  </dependency>
  <dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming_2.11</artifactId>
    <version>2.4.0</version>
  </dependency>
  <dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming-kafka-0-10_2.11</artifactId>
    <version>2.4.0</version>
  </dependency>
  <dependency>
    <groupId>org.json</groupId>
    <artifactId>json</artifactId>
    <version>20180813</version>
  </dependency>
</dependencies>
<repositories>
  <id>SparkPackagesRepo</id>
  <url>http://dl.bintray.com/spark-packages/maven</url>
</repositories>
``` 

+ java main 클래스 코드 예제 

``` C
import org.apache.spark.sql.SparkSession;

public class SimpleExample{
  public static void main(String[] args){
    SparkSession spark = SparkSession.builder().getOrCreate();
    spark.range(1,2000).count();
  }
}
``` 
+ 애플리케이션 실행 
``` C
$SPARK_HOME/bin/spark-submit \
  --class com.databricks.example.SimpleExample \
  --master local \
  target/spark-example-0.1-SNAPSHOT.jar "hello"
``` 


## 16.2 스파크 애플리케이션 테스트 
- 스파크 애플리케이션을 개발하고 실행하는 방법 뿐만이 아닌, 애플리케이션 "테스트"에 대한 몇가지 핵심 원칙 및 구성 전략 알아보기!

### 전략적 원칙
+ 입력 데이터에 대한 유연성 
  + 데이터 파이프라인은 다양한 유형의 입력 데이터에 유연하게 대처할 수 있어야 함 
  + 입력 데이터 중 일부가 변하더라도 유연하게 대처할 수 있어야 함 
  + 입력 데이터로 인해 발생할 수 있는 다양한 예외 상황 테스트하는 코드 작성 필요 
+ 비즈니스 로직 변경에 대한 유연성
  + 파이프라인 내부의 비즈니스 로직이 바뀔수도 있음
  + 예상했던 원형 데이터의 형태가 실제 원형 데이터와 같은지 확인하고 싶음 
  + 실제와 유사한 데이터를 사용해 비즈니스 로직을 꼼꼼히 테스트 해야 함 -> 스파크 단위 테스트가 아닌 비즈니스 로직 테스트! 
+ 결과의 유연성과 원자성
  + 입력 데이터 & 비즈니스 로직의 테스트 완료되었다면, 결과가 의도한대로 반환하는지 확인해야 함 
  + 결과 데이터가 스키마에 맞는 적절한 형태로 반환 
  + 대부분의 스파크 파이프라인은 다른 스파크 파이프라인의 입력으로 사용됨
  + 따라서, 다음 스파크 파이프라인에서의 '상태' (얼마나 자주 갱신되는지, 늦게 들어온 데이터가 없는지, 마지막 순간에 변경되지 않았는지)등을 알 수 있어야 함 

### 테스트 코드 작성 시 고려사항 
- 애플리케이션 테스트를 쉽게 만들어주는 테스트 구성 전략
- 적절한 단위 테스트를 작성해 입력 데이터나 구조가 변경되어도 비즈니스 로직이 정상적으로 동작하는지 확인해야 함 
- 단위 테스트 시 스키마가 변경되는 상황에 대해 쉽게 대응할 수 있음 
+ Spark Session 관리하기 
  +  스파크 코드에서 의존성 주입 방식으로 SaprkSession을 관리하도록 만들어야 함
  +  즉, SaprkSession을 한번만 초기화 하고 런타임 환경에서 함수와 클래스에 전달하는 방식을 사용하면 테스트 중에 SaprkSession으로 개별 함수를 쉽게 테스트 할 수 있음 
+ 테스트 코드용 스파크 API 선정하기
  + 스파크는 SQL, DataFrame, Dataset등 다양한 API 제공 
    + 개발 속도 올리기 위한다면 덜 엄격한 SQL, Dataframe API 사용
    + 타입 안정성을 얻기 위해 Dataset과 RDD API 사용 
  + API유형에 상관 없이 각 함수의 입력과 출력 타입을 문서로 만들고 테스트 필요 
    + 안정성 API를 사용하면 함수가 가지고 있는 최소한의 규약 (함수 시그니처)를 지켜야 하므로 다른 코드에서 재사용하기 쉬움 
  + 적절한 언어 선택도 필요
    + 대규모 애플리케이션이나 저수준 API를 사용해 성능을 완전히 제어하려면 스칼라, 자바와 같은 정적 데이터타입 언어 사용 
    + 파이썬이나 R은 각 언어가 제공하는 강력한 라이브러리를 활용하려는 경우에 적합 

### 

