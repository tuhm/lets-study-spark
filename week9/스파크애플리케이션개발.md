
# 16장. 스파크 애플리케이션 개발하기  

-이전 장! 스파크가 클러스터 환경에서 어떻게 스파크 코드를 실행하는지
-이번 장! 표준 스파크 애플리케이션을 개발하고 클러스터에 배포하는 과정 

## 16.1 스파크 애플리케이션 작성하기 
-스파크 애플리케이션은 스파크 클러스터와 사용자 코드 두가지 조합으로 구성 

### 간단한 스칼라 기반 앱 
+ 스파크 애플리케이션은 자바가상머신 기반의 빌드 도구인 sbt나 아파치 메이븐을 이용해 빌드 
  + sbt를 사용하는 것이 더 쉬움 
  + sbt 빌드 환경 구성을 위해 build.sbt파일 정의 필요 
    + 프로젝트 메타데이터(패키지명, 패키지 버전 정보 등)
    + 라이브러리 의존성을 관리하는 장소 
    + 라이브러리에 포함된 의존성 정보 
  + sbt 홈페이지에서 관련 정보 찾기 가능 : www.scala-sbt.org/1.x/docs/Basic-Def.html

``` C
name := "example" // change to project name
organization := "com.databricks" // change to your org
version := "0.1-SNAPSHOT"
scalaVersion := "2.11.8"

// Spark Information
val sparkVersion = "2.1.0"

// allows us to include spark packages
resolvers += "bintray-spark-packages" at
  "https://dl.bintray.com/spark-packages/maven/"

resolvers += "Typesafe Simple Repository" at
  "http://repo.typesafe.com/typesafe/simple/maven-releases/"

resolvers += "MavenRepository" at
  "https://mvnrepository.com/"

libraryDependencies ++= Seq(
  // spark core
  "org.apache.spark" %% "spark-core" % sparkVersion,
  "org.apache.spark" %% "spark-sql" % sparkVersion,

  // spark-modules
  "org.apache.spark" %% "spark-graphx" % sparkVersion,
  // "org.apache.spark" %% "spark-mllib" % sparkVersion,

  // spark packages
  "graphframes" % "graphframes" % "0.4.0-spark2.1-s_2.11",

  // testing
  "org.scalatest" %% "scalatest" % "2.2.4" % "test",
  "org.scalacheck" %% "scalacheck" % "1.12.2" % "test",

  // logging

"org.apache.logging.log4j" % "log4j-api" % "2.4.1",
  "org.apache.logging.log4j" % "log4j-core" % "2.4.1"
) 
```

+ build.sbt파일 정의 후 sbt메뉴얼의 포준 스칼라 프로젝트 구조 작성 ( 메이븐 프로젝트도 동일 구조)
    
    
  ``` C
  src/
  main/
    resources/
       <JAR 파일에 포함할 파일들>
    scala/
       <메인 스칼라 소스 파일>
    java/
       <메인 자바 소스 파일>
  test/
    resources
       <테스트 JAR에 포함할 파일들>
    scala/
       <테스트용 스칼라 소스 파일>
    java/
       <테스트용 자바 소스 파일> ```  
       
+ 스칼라와 자바 디렉터리에 아래 소스코드 작성 
  + SparkSession을 초기화 -> 애플리케이션 실행 -> 종료 
    
 ``` C
    object DataFrameExample extends Serializable {
  def main(args: Array[String]) = {

    val pathToDataFolder = args(0)

    // 설정값 지정 -> SparkSession 시작
    val spark = SparkSession.builder().appName("Spark Example")
      .config("spark.sql.warehouse.dir", "/user/hive/warehouse")
      .getOrCreate()

    // udf 등록
    spark.udf.register("myUDF", someUDF(_:String):String)
    
    val df = spark.read.json(pathToDataFolder + "data.json")
    val manipulated = df.groupBy(expr("myUDF(group)")).sum().collect()
     .foreach(x => println(x))
  }
}  

```

+ spark-submit 명령을 사용해 예제 코드를 클러스터에 제출 
+ 실행 명령 중 main 클래스 지정한 부분 중요! 
+ 하나의 jAR 파일 안에 관련 라이브러리 모두를 포함하는 uber-jar 또는 fat-jar로 빌드하기 위해 sbt assemble 명령 사용 
+ 라이브러리 의존성 충돌하는 경우가 생길 수 있음 
+ 가장 쉬운 빌드 방법은 sbt package 명령을 실행하는 것 -> 관련 라이브러리 모두를 target 폴더에 모아줌 
  + 간단한 예제sms 필요한 library가 하나 뿐이지만, 보통의 경우 하나의 Application을 개발하기 위해서는 상당히 많은 library를 사용하게 됨 
  + 이런 Application을 배포하기 위해서는 위에서처럼, 사용된 library의 jar 파일과 classpath를 함께 제공해 주어야 하는데, 몇 십개씩되는 library를 일일이 관리하는 것은 어려움 
  + 따라서 .sbt파일에 addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.4") 플러그인 이름과 버전 추가 
  + shell에서 assembly라는 명령어 사용 
  + jar tf test-assembly-1.0.jar내용 확인 -> fat jar로 불림 
    + Application을 구동하는데 필요한 Class 파일들을 하나의 jar 내부에 모두 가지고 있음 

``` C

jae/
jae/lee/
jae/lee/example/
scala/
scala/annotation/
scala/annotation/meta/
scala/annotation/unchecked/
scala/beans/
scala/collection/
scala/collection/concurrent/
scala/collection/convert/
scala/collection/generic/
<중략>

``` 

+ target 폴더에 saprk-submit이라는 인수로 사용할 jar 파일이 들어 있음 
+ 스칼라 패키지 빌드 후, saprk-submit 명령을 사용해 로컬머신에서 실행 가능 

``` C
$SPARK_HOME/bin/spark-submit \
  --class com.databricks.example.DataFrameExample \
  --master local \
  target/scala-2.11/example_2.11-0.1-SNAPSHOT.jar "hello"
``` 

### 파이썬 애플리케이션 작성 
+  일반 파이썬 애플리케이션이나 패키지를 작성하는 방법과 유사 
+  스파크에는 빌드 개념이 없으며, Pyspark 애플리케이션은 파이썬 스크립트와 동일 
  +  애플리케이션 실행 = 스크립트 실행
+ 코드 재사용을 위해 여러 파이썬 파일을 하나의 egg나 ZIP 파일 형태로 압축 
+ spark-submit의 --py-files 인수로 .py, .zip, .egg파일을 지정하면 배포 가능 
1. '스칼라나 자바의 메인 클래스' 역할을 하는 파이썬 파일 작성 (sparkSession을 생성하는 실행 가능한 스크립트 파일) 
``` C
# 파이썬 코드
from __future__ import print_function

if __name__ == '__main__':
  from pyspark.sql import SparkSession
  spark = SparkSession.builder \
    .master("local") \
    .appName("Word Count") \
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()
  
  print(spark.range(5000).where("id>500").selectExpr("sum(id)").collect())
```
2. 라이브러리 의존성을 정의하기 위해 pip 사용 가능 (pip install pyspark 명령으로 설치 -> 다른 패키지와 같은 방식으로 의존성 정의)
3. 애플리케이션 실행 
``` C
$SPARK_HOME/bin/spark-submit --master local pyspark_template/main.py
``` 
### 자바 애플리케이션 작성 
+ 스칼라의 작성 방법과 매우 유사 
+ 차이점은 사용자가 메이븐을 사용해 라이브러리 의존성을 지정
  + 메이븐 형식 
``` C
<dependencies>
  <dependency>
    <groupId>junit</groupId>
    <artifactId>junit</artifactId>
    <version>3.8.1</version>
    <scope>test</scope>
  </dependency>
  <dependency> <!-- Spark dependency -->
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-sql_2.11</artifactId>
    <version>2.4.0</version>
  </dependency>
  <dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming_2.11</artifactId>
    <version>2.4.0</version>
  </dependency>
  <dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming-kafka-0-10_2.11</artifactId>
    <version>2.4.0</version>
  </dependency>
  <dependency>
    <groupId>org.json</groupId>
    <artifactId>json</artifactId>
    <version>20180813</version>
  </dependency>
</dependencies>
<repositories>
  <id>SparkPackagesRepo</id>
  <url>http://dl.bintray.com/spark-packages/maven</url>
</repositories>
``` 

+ java main 클래스 코드 예제 

``` C
import org.apache.spark.sql.SparkSession;

public class SimpleExample{
  public static void main(String[] args){
    SparkSession spark = SparkSession.builder().getOrCreate();
    spark.range(1,2000).count();
  }
}
``` 
+ 애플리케이션 실행 
``` C
$SPARK_HOME/bin/spark-submit \
  --class com.databricks.example.SimpleExample \
  --master local \
  target/spark-example-0.1-SNAPSHOT.jar "hello"
``` 


## 16.2 스파크 애플리케이션 테스트 
- 스파크 애플리케이션을 개발하고 실행하는 방법 뿐만이 아닌, 애플리케이션 "테스트"에 대한 몇가지 핵심 원칙 및 구성 전략 알아보기!

### 전략적 원칙
+ 입력 데이터에 대한 유연성 
  + 데이터 파이프라인은 다양한 유형의 입력 데이터에 유연하게 대처할 수 있어야 함 
  + 입력 데이터 중 일부가 변하더라도 유연하게 대처할 수 있어야 함 
  + 입력 데이터로 인해 발생할 수 있는 다양한 예외 상황 테스트하는 코드 작성 필요 
+ 비즈니스 로직 변경에 대한 유연성
  + 파이프라인 내부의 비즈니스 로직이 바뀔수도 있음
  + 예상했던 원형 데이터의 형태가 실제 원형 데이터와 같은지 확인하고 싶음 
  + 실제와 유사한 데이터를 사용해 비즈니스 로직을 꼼꼼히 테스트 해야 함 -> 스파크 단위 테스트가 아닌 비즈니스 로직 테스트! 
+ 결과의 유연성과 원자성
  + 입력 데이터 & 비즈니스 로직의 테스트 완료되었다면, 결과가 의도한대로 반환하는지 확인해야 함 
  + 결과 데이터가 스키마에 맞는 적절한 형태로 반환 
  + 대부분의 스파크 파이프라인은 다른 스파크 파이프라인의 입력으로 사용됨
  + 따라서, 다음 스파크 파이프라인에서의 '상태' (얼마나 자주 갱신되는지, 늦게 들어온 데이터가 없는지, 마지막 순간에 변경되지 않았는지)등을 알 수 있어야 함 

### 테스트 코드 작성 시 고려사항 
- 애플리케이션 테스트를 쉽게 만들어주는 테스트 구성 전략
- 적절한 단위 테스트를 작성해 입력 데이터나 구조가 변경되어도 비즈니스 로직이 정상적으로 동작하는지 확인해야 함 
- 단위 테스트 시 스키마가 변경되는 상황에 대해 쉽게 대응할 수 있음 
+ Spark Session 관리하기 
  +  스파크 코드에서 의존성 주입 방식으로 SaprkSession을 관리하도록 만들어야 함
  +  즉, SaprkSession을 한번만 초기화 하고 런타임 환경에서 함수와 클래스에 전달하는 방식을 사용하면 테스트 중에 SaprkSession으로 개별 함수를 쉽게 테스트 할 수 있음 
+ 테스트 코드용 스파크 API 선정하기
  + 스파크는 SQL, DataFrame, Dataset등 다양한 API 제공 
    + 개발 속도 올리기 위한다면 덜 엄격한 SQL, Dataframe API 사용
    + 타입 안정성을 얻기 위해 Dataset과 RDD API 사용 
  + API유형에 상관 없이 각 함수의 입력과 출력 타입을 문서로 만들고 테스트 필요 
    + 안정성 API를 사용하면 함수가 가지고 있는 최소한의 규약 (함수 시그니처)를 지켜야 하므로 다른 코드에서 재사용하기 쉬움 
  + 적절한 언어 선택도 필요
    + 대규모 애플리케이션이나 저수준 API를 사용해 성능을 완전히 제어하려면 스칼라, 자바와 같은 정적 데이터타입 언어 사용 
    + 파이썬이나 R은 각 언어가 제공하는 강력한 라이브러리를 활용하려는 경우에 적합 

### 단위 테스트 프레임워크에 연결하기 
- 코드를 단위 테스트하려면 각 언어의 표준 프레임워크를 사용하고, 테스트 하네스에서 테스트마다 SparkSession을 생성하고 제거하도록 설정하는 것이 좋음 
- 각 프레임워크는 SparkSession 새성과 제거를 수행할 수 있는 메커니즘 (before, after) 제ㅗㄱㅇ 

### 데이터 소스 연결하기
+ 테스트 코드에서는 운영 환경의 데이터 소스에 접속하지 말아야 함 
  + 데이터 소스가 변경되더라도 고립된 환경에서 개발자가 쉽게 테스트 코드를 실행할 수 있음
+ 비즈니스 로직을 가진 함수가 데이터소스에 직접 접근하지 않고 DataFrame이나 Dataset을 넘겨받게 만들기 
+ 이렇게 생성된 함수를 재사용하는 코드는 데이터 소스의 종류에 상관없이 같은 방식으로 동작 
+ 스파크의 구조적 API를 사용하는 경우 이름이 지정된 테이블을 이용해 문제를 해결할 수 있음
  + 몇개의 더미 데이터셋(작은 테스트 파일이나 인메모리 객체)에 이름을 붙여 테이블로 등록하고 사용 

## 16.3 개발 프로세스 
1. 대화형 노트북이나 유사한 환경에 초기화된 작업 공간 마련 
2. 핵심 컴포넌트와 알고리즘을 만든 후 라이브러리나 패키지 같은 영구적인 영역으로 코드를 옮김
3. 로컬머신의 경우 spark-shell을 활용해 애플리케이션 개발에 활용하는것이 가장 적합한 방식 (대부분은 개발에 사용하지만 spark-submit 명령은 스파크 클러스터에 운영용 애플리케이션을 실행하기 위해 사용 )
4. spark-submit 명령어로 클러스터에 애플리케이션 제출 

## 16.4 애플리케이션 시작하기 
- spark-submit 명령에 옵션, 애플링케이션 JAR 파일 또는 스크립트와 관련된 인수 지정해 사용
``` C
./bin/spark-submit \
  --class <메인 클래스> \
  --master <스파크 마스터 URL> \
  --deploy-mode <배포 모드 - cluster, client> \
  --conf <키>=<값> \
  ... # other options
  <애플리케이션 JAR 또는 스크립트> \
  [애플리케이션 args]
``` 
+ 명령어로 스파크잡을 제출할 때에는 클라이언트 모드와 클러스터 모드중 하나를 선택해야 함 
+ 드라이버와 익스큐터간의 지연 시간을 줄이기 위해 클러스터 모드로 실행한 것을 추천 
+ 파이썬의 경우 .jar 파일의 위치에 .py 파일을 지정하고 파이썬, zip, egg, .py 파일을 --py-files에 추가함 
+ spark-submit에서 사용하는 옵션 정보 
![image](https://user-images.githubusercontent.com/18010639/133051310-4d241a32-64b0-449b-ae63-2f5840f4a5c0.png)

+ 특정 베포 환경에서 사용할 수 있는 설정 
![image](https://user-images.githubusercontent.com/18010639/133052247-5473ce19-c352-4064-bbb4-afdbea95dead.png)

### 애플리케이션 시작 예제 
- 로컬 머신에서 SparkPi 클래스를 메인 클래스로 사용해 테스트

``` C
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://207.184.161.138:7077 \
  --executor-memory 20G \
  --total-executor-cores 100 \
  replace/with/path/to/examples.jar \
  1000
``` 
- 파이썬 예제 (로컬 모드)
``` C
$SPARK_HOME/bin/spark-submit \
  --master spark://207.184.161.138:7077 \
  examples/src/main/python/pi.py \
  1000
``` 

## 16.5 애플리케이션 환경 설정하기 
- 스파크는 다양한 환경 설정 제공
  - 애플리케이션 속성
  - 런타임 환경
  - 셔플 동작 방식
  - 스파크 UI
  - 압축과 직렬화 
  - 메모리 관리 
  - 처리 방식
  - 네트워크 설정
  - 스케줄링
  - 동적 할당
  - 보안 
  - 암호화
  - 스파크 SQL
  - 스파크 스트리밍
  - SparkR
  
- Spark 시스템 설정 방법
+ Spark 속성: 대부분의 어플리케이션 파라미터를 제어, SparkConf 객체를 사용해 설정 가능
+ Java 시스템 속성
+ 하드코딩된 환경 설정 파일 : xpavmffltdml thrtjdrkqtdmf gkemzheld 

### SparkConf 
- SparkConf는 애플리케이션의 모든 설정 관리 
- import 구문 지정 후 객체 생성 -> 불변성 
``` C
import org.apache.spark.SparkConf

val conf = new SparkConf()
  .setMaster("local[2]")
  .setAppName("DefinitiveGuide")
  .set("some.conf", "to.some.value")
```
+ SparkConf객체는 개별 스파크 애플리케이션에 대한 스파크 속성값을 구성하는 용도로 사용 
+ 스파크 속성값은 스파크 애플리케이션의 동작 방식과 클러스터 구성 방식을 제어 
  + 예제 : 로컬 클러스터에 2개의 스레드 생성, 스파크 UI에 표시할 애플케이션의 이름 정의 
+ 설정값은 명령행 인수를 통해 런타임에 구성할수 있음 
``` C
$SPARK_HOME/bin/spark-submit --name "DefinitiveGuide" --master local[4] ...
``` 
+ 시간 주기 형태의 속성값 정의할때 사용하는 포맷 
  + 25ms(밀리세컨드)
  + 5s(초)
  + 10m 또는 10min(분)
  + 3h(시간)
  + 5d(일)
  + ly(년) 

### 애플리케이션 속성 
- 애플리케이션 속성은 spark-submit 명령이나 스파크 애플리케이션을 개발할 때 설정할 수 있음 
- 현재 지원하는 애플리케이션 속성 목록 
![image](https://user-images.githubusercontent.com/18010639/133055159-7aaede5e-8e31-4f9a-b9c2-469a4b43413f.png)


+ 들아ㅣ버가 실행된 호스트의 4040포트로 접속 후 스파크 UI의 'Environment' 탭을 확인해 값이 올바르게 설정되었는지 확인할 수 있음 
+ spark-defaults.conf, SparkConf, 명령행에서 지정한 값을 확인할 수 있음 

### 런타임 속성 
- 드물지만 애플리케이션의 런타임 환경을 설정해야 할 수도 있음 
- 드라이버와 익스큐터를 위한 추가 클래스패스와 파이썬패스, 파이썬 워커 설정 , 다양한 로그 관련 속성을 정의할 수 있음 
- spark.apache.org/docs/latest/configuration.html#runtime-environment


### 실행 속성 
- 실제처리를 더욱 세밀하게 제어할 수 있기 때문에  자주 사용 
- 자주 사용하는 속성으로 spark.executor.cores, spark.files.maxPartitionBytes 존재 
- spark.apache.org/docs/latest/configuration.html#runtime-environment

### 메모리 관리 설정 
- 사용자 애플리케이션을 최적화하기 위해 메모리 옵션을 수동으로 관리해야 할 때가 있음 
- 대다수의 메모리 설정은 자동 관리 기능의 추가로 인해 스파크2.x버전에서 제거 -> 신경쓰지 않아도 됨 

### 셔플 동작방식 설정
- 15장에서 네트워크 부하 때문에 스파크 잡에서 셔플이 얼마나 큰 병목 구간이 될 수 있는지를 살펴 봄
- 셔플 동작 방식을 제어하기 위한 고급 설정 존재 

### 환경 변수 
- 스파크가 설치된 디렉터리의 conf/spark-env.sh 파일에서  읽은 환경변수로 특정 스파크 설정을 구성할 수 있음 
- conf/spark-env.sh.template 파일을 복사해서 생성할  수 있음
  - 복사된 파일에 반드시 실행 권한을 부여해야 함 
- spark-env.sh 스크립트에 다음과 같은 변수 설정 가능 
+ JAVA_HOME
  + Java 가 설치된 경로(기본 PATH에 자바 경로가 포함되지 않는 경우)
+ PYSPARK_PYTHON
 + PySpark 의 드라이버와 워커 모두에 사용할 Python 실행 명령 지정(default: python2.7)
+ spark.pyspark.python 속성: PYSPARK_PYTHON 보다 높은 우선순위를 가짐
+ PYSPARK_DRIVER_PYTHON
  + 드라이버에서 PySpark 를 사용하기 위해 실행 가능한 Python 바이너리 지정
+ Default: PYSPARK_PYTHON
+ spark.pyspark.driver.python 속성: PYSPARK_DRIVER_PYTHON보다 높은 우선순위를 가짐
+ SPARKR_DRIVER_R
  + SparkR 쉘에서 사용할 R 바이너리 실행 명령 지정
+ Default: R
+ spark.r.shell.command 속성: SPARKR_DRIVER_R보다 높은 우선순위를 가짐
+ SPARK_LOCAL_IP
  + 머신의 IP 주소 지정
+ SPARK_PUBLIC_DNS
  + Spark 프로그램이 다른 머신에 알려줄 호스트명 


### 애플리케이션에서 잡 스케줄링
- 별도의 스레드를 사용해 여러 잡을 동시에 실행할 수 있음 
- 잡? 해당 액션을 수행하기 위해 실행되어야 할 모든 테스크와 스파크 액션을 의미 
- 스파크의 스케줄러는 스레드 안정성을 충분히 보장함 
- 여러 요청 (예. 다수의 사용자가 쿼리를 요청하는 경우)를 동시에 처리할 수 있는 애플리케이션을 만들 수 있음 
+ 스파크의 스케줄러는 FIFO 방식으로 동적
  + 큐의 head에 있는 잡이 클러스터의 전체 자원을 사용하지 않으면 이후 잡을 바로 실행할 수 있음. 반면 큐의 head의 잡이 너무 크면 이후 잡은 아주 늦게 실행됨
+ 여러 스파크 잡이 자원을 공평하게 나눠 쓰도록 구성할 수 있음 
  + 슿파크는 모든 잡이 클러스터 자원을 거의 동일하게 사용하도록 라운드 로빈 방식으로 여러 스파크잡의 태스크를 할당함 
  + 장시간 수행되는 스파크 잡이 처리되는 중에 짧게 끝난 스파크 잡이 제출된 경우 즉시 장시간 수행되는 스파크 잡의 자원을 할당받아 처리 
  + 장시간 수행되는 스파크 잡의 종료를 기다리지 않고 빠르게 응답할 수 있음 
+ 페어 스케줄러 (spark.scheduler.mode = FAIR)
 + 여러개의 잡을 풀로 그룹화하는 방식도 지원함 
 + 개별 풀에 다른 스케줄링 옵션이나 가중치를 설정할 수 있음 
 + 우선순위가 높은 풀을 설정할 수 있음 
 + 하둡의 페어 스케줄러 모델을 본떠서 만듦
 + 사용자가 명시적으로 풀을 지정하지 않으면 Spark 는 새로운 잡을 default 풀에 할당
 + 잡을 제출하는 스레드에서 SparkContext 의 로컬 속성인 spark.scheduler.pool 속성을 설정해 풀을 지정 
``` C
sc.setLocalProperty("spark.scheduler.pool", "pool1")
``` 
  + 이 스레드에서 제출하는 모든 잡은 이 풀을 사용
  + 이 설정은 사용자를 대신해 스레드가 여러 잡을 쉽게 실행할 수 있도록 스레드별로 지정 가능
  + 스레드에 연결된 풀을 초기화하고 싶을 경우 spark.scheduler.pool 속성의 값을 null 로 지정
